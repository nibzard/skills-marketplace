#!/usr/bin/env python3
"""
Endpoint Enumeration Tool
Discovers URLs, API endpoints, and attack surfaces from web applications
"""

import sys
import re
import json
import time
import urllib.parse
import urllib.request
import ssl
from urllib.error import URLError

class EndpointEnumerator:
    def __init__(self, base_url):
        self.base_url = base_url.rstrip('/')
        self.discovered = set()
        self.session = self._create_session()
        self.user_agent = 'Mozilla/5.0 (compatible; Pentest-Toolkit/1.0)'

    def _create_session(self):
        """Create a simple session-like object without external dependencies"""
        ctx = ssl.create_default_context()
        ctx.check_hostname = False
        ctx.verify_mode = ssl.CERT_NONE
        return {'context': ctx}

    def _make_request(self, url):
        """Make HTTP request without external dependencies"""
        try:
            req = urllib.request.Request(url)
            req.add_header('User-Agent', self.user_agent)
            req.add_header('Accept', 'text/html,application/json,*/*')

            with urllib.request.urlopen(req, context=self.session['context'], timeout=10) as response:
                return {
                    'status': response.status,
                    'headers': dict(response.headers),
                    'content': response.read().decode('utf-8', errors='ignore')
                }
        except Exception as e:
            return {'error': str(e)}

    def extract_from_html(self, content, url):
        """Extract endpoints from HTML content"""
        endpoints = set()

        # Extract URLs from various HTML patterns
        patterns = [
            r'href=[\'"]([^\'"]+)[\'"]',
            r'action=[\'"]([^\'"]+)[\'"]',
            r'src=[\'"]([^\'"]+)[\'"]',
            r'url:[\'"]([^\'"]+)[\'"]',
            r'fetch\([\'"]([^\'"]+)[\'"]',
            r'axios\.[^\\(]*\([\'"]([^\'"]+)[\'"]',
            r'/api/[^\\s\\\'\\"<>]+',
            r'/v[0-9]/[^\\s\\\'\\"<>]+',
        ]

        for pattern in patterns:
            matches = re.findall(pattern, content, re.IGNORECASE)
            for match in matches:
                if self._is_valid_endpoint(match):
                    endpoints.add(self._normalize_url(match, url))

        return endpoints

    def extract_from_javascript(self, content, url):
        """Extract API endpoints from JavaScript code"""
        endpoints = set()

        # Common API patterns
        patterns = [
            r'[\'"](/api/[^\'"]+)[\'"]',
            r'[\'"](/v[0-9]/[^\'"]+)[\'"]',
            r'endpoint:\s*[\'"]([^\'"]+)[\'"]',
            r'url:\s*[\'"]([^\'"]+)[\'"]',
            r'path:\s*[\'"]([^\'"]+)[\'"]',
            r'route:\s*[\'"]([^\'"]+)[\'"]',
        ]

        for pattern in patterns:
            matches = re.findall(pattern, content, re.IGNORECASE)
            for match in matches:
                if self._is_valid_endpoint(match):
                    endpoints.add(self._normalize_url(match, url))

        return endpoints

    def _is_valid_endpoint(self, endpoint):
        """Check if endpoint looks like a valid URL path"""
        # Skip external URLs, anchors, and common non-endpoints
        skips = ['javascript:', 'mailto:', '#', 'tel:', 'data:', 'void(0)']

        if any(skip in endpoint.lower() for skip in skips):
            return False

        # File extensions that are usually not endpoints
        skip_extensions = ['.css', '.js', '.png', '.jpg', '.gif', '.ico', '.pdf']
        if any(endpoint.lower().endswith(ext) for ext in skip_extensions):
            return False

        return True

    def _normalize_url(self, endpoint, base_url):
        """Normalize URL to full path"""
        if endpoint.startswith('http'):
            return endpoint

        if endpoint.startswith('//'):
            return 'https:' + endpoint

        parsed = urllib.parse.urlparse(base_url)
        base = f"{parsed.scheme}://{parsed.netloc}"

        if endpoint.startswith('/'):
            return base + endpoint
        else:
            return base + '/' + endpoint

    def brute_common_paths(self):
        """Test common paths and endpoints"""
        common_paths = [
            # Admin and management
            '/admin', '/admin/login', '/admin/panel', '/administrator',
            '/wp-admin', '/phpmyadmin', '/manager', '/admin.php',

            # API endpoints
            '/api', '/api/v1', '/api/v2', '/v1', '/v2',
            '/api/auth', '/api/users', '/api/data',

            # Common web paths
            '/login', '/logout', '/register', '/profile', '/settings',
            '/dashboard', '/home', '/index', '/main',

            # Testing paths (like those found in Apex)
            '/xss1', '/xss2', '/xss3', '/test', '/debug',
            '/health', '/status', '/ping', '/info',

            # File operations
            '/upload', '/download', '/files', '/static', '/media',

            # Documentation
            '/docs', '/documentation', '/help', '/support',

            # Config and backup
            '/config', '/backup', '/.git', '/.env', '/phpinfo',
        ]

        discovered = set()
        for path in common_paths:
            url = self.base_url + path
            response = self._make_request(url)

            if response.get('status') in [200, 301, 302, 403]:
                discovered.add(url)
                print(f"[+] Found: {url} (Status: {response.get('status')})")

        return discovered

    def crawl_site(self, max_pages=50):
        """Crawl the website to discover endpoints"""
        visited = set()
        to_visit = {self.base_url}

        while to_visit and len(visited) < max_pages:
            url = to_visit.pop()
            if url in visited:
                continue

            visited.add(url)
            print(f"[*] Crawling: {url}")

            response = self._make_request(url)
            if 'error' in response:
                continue

            content = response['content']

            # Extract endpoints based on content type
            content_type = response['headers'].get('content-type', '').lower()

            if 'html' in content_type:
                endpoints = self.extract_from_html(content, url)
            elif 'json' in content_type or 'javascript' in content_type:
                endpoints = self.extract_from_javascript(content, url)
            else:
                endpoints = self.extract_from_html(content, url)  # Try anyway

            for endpoint in endpoints:
                if endpoint not in visited:
                    to_visit.add(endpoint)
                    self.discovered.add(endpoint)

        return self.discovered

    def test_enumerated_patterns(self):
        """Test for numbered endpoints and common patterns"""
        discovered = set()

        # Pattern 1: Numbered endpoints (like /xss1, /xss2, etc.)
        base_patterns = ['/xss', '/test', '/page', '/item', '/id', '/user']

        for pattern in base_patterns:
            for i in range(1, 11):  # Test 1-10
                url = f"{self.base_url}{pattern}{i}"
                response = self._make_request(url)

                if response.get('status') in [200, 301, 302]:
                    discovered.add(url)
                    print(f"[+] Found numbered endpoint: {url}")

        # Pattern 2: Common API endpoints with IDs
        api_patterns = ['/api/users/', '/api/posts/', '/api/data/', '/api/items/']

        for pattern in api_patterns:
            for id in [1, 2, 999, 123]:  # Test common IDs
                url = f"{self.base_url}{pattern}{id}"
                response = self._make_request(url)

                if response.get('status') == 200:
                    discovered.add(url)
                    print(f"[+] Found API endpoint: {url}")

        return discovered

    def run_enumeration(self):
        """Run comprehensive endpoint enumeration"""
        print(f"[*] Starting endpoint enumeration for: {self.base_url}")
        print("=" * 60)

        all_endpoints = set()

        # Method 1: Web crawling
        print("[1/4] Crawling website...")
        crawled = self.crawl_site()
        all_endpoints.update(crawled)
        print(f"    Found {len(crawled)} endpoints from crawling")

        # Method 2: Brute force common paths
        print("[2/4] Testing common paths...")
        brute_forced = self.brute_common_paths()
        all_endpoints.update(brute_forced)
        print(f"    Found {len(brute_forced)} common paths")

        # Method 3: Test enumerated patterns
        print("[3/4] Testing numbered patterns...")
        patterns = self.test_enumerated_patterns()
        all_endpoints.update(patterns)
        print(f"    Found {len(patterns)} pattern-based endpoints")

        # Method 4: Test HTTP methods on discovered endpoints
        print("[4/4] Testing HTTP methods...")
        method_endpoints = self.test_http_methods(all_endpoints)
        all_endpoints.update(method_endpoints)

        # Remove the base URL from results (not an endpoint)
        all_endpoints.discard(self.base_url)

        print("\n" + "=" * 60)
        print(f"[*] Total unique endpoints discovered: {len(all_endpoints)}")
        print("\nDiscovered endpoints:")

        for endpoint in sorted(all_endpoints):
            print(f"  - {endpoint}")

        return list(all_endpoints)

    def test_http_methods(self, endpoints):
        """Test different HTTP methods on discovered endpoints"""
        new_endpoints = set()

        for endpoint in list(endpoints)[:10]:  # Limit to avoid too many requests
            for method in ['POST', 'PUT', 'DELETE', 'OPTIONS', 'PATCH']:
                try:
                    req = urllib.request.Request(endpoint, method=method)
                    req.add_header('User-Agent', self.user_agent)

                    with urllib.request.urlopen(req, context=self.session['context'], timeout=5) as response:
                        if response.status not in [404, 405]:
                            new_endpoints.add(f"{endpoint} ({method})")
                            print(f"    {method} {endpoint}: {response.status}")
                except:
                    pass  # Expected for unsupported methods

        return new_endpoints

def main():
    if len(sys.argv) != 2:
        print("Usage: python3 enumerate_endpoints.py <base_url>")
        print("Example: python3 enumerate_endpoints.py https://example.com")
        sys.exit(1)

    base_url = sys.argv[1]

    # Validate URL format
    if not (base_url.startswith('http://') or base_url.startswith('https://')):
        base_url = 'https://' + base_url

    enumerator = EndpointEnumerator(base_url)

    try:
        endpoints = enumerator.run_enumeration()

        # Save results to JSON
        output_file = 'discovered_endpoints.json'
        with open(output_file, 'w') as f:
            json.dump({
                'target': base_url,
                'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
                'endpoints': endpoints,
                'count': len(endpoints)
            }, f, indent=2)

        print(f"\n[+] Results saved to: {output_file}")

    except KeyboardInterrupt:
        print("\n[!] Enumeration interrupted by user")
        sys.exit(1)
    except Exception as e:
        print(f"\n[!] Error: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()